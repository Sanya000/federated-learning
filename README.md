# federated-learning
A Python project implementing next word prediction using LSTM networks in a federated learning setting on a number of Raspberry Pi's to explore the effect of heterogeneous data distribution and strategies to mitigate it.
# Abstract
Federated learning (FL) is a promising approach for training machine learning models in the Internet of Things (IoT) setting, where data is distributed across multiple devices. However, FL faces challenges such as heterogenous data distributions which induce a parameter shift in the trained models. This dissertation discusses this issue in depth and implements a number of strategies to mitigate parameter shift. The model implemented performs next word prediction on the Reddit dataset, and is implemented on Raspberry Pi’s to simulate a real world FL deployment. Different parameters are experimented with to find a heuristically optimal baseline model which is compared to different configurations of Federated Batch Normalization (FedBN), Federated Proximal (FedProx) and federated optimizers. The results indicate that there is severe overfitting present in the FL system set-up and the parameter shift mitigations prove to be inconclusive in their effectiveness as their effect is overshadowed by the model’s overfitting. Increasing the number of clients in the FL system to at least above 10 is necessary to get an acceptable performance of the model and witness the effect of parameter shift mitigations.

